{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first notebook, we will go through the basics of using the SDK to:\n",
    " - Spin up a Ray cluster with our desired resources\n",
    " - View the status and specs of our Ray cluster\n",
    " - Take down the Ray cluster when finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pieces from codeflare-sdk\n",
    "from codeflare_sdk.cluster.cluster import Cluster, ClusterConfiguration\n",
    "from codeflare_sdk.job.jobs import DDPJobDefinition\n",
    "# Ignoring auth for now"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we want to define our cluster by specifying the resources we require for our batch workload. Below, we define our cluster object (which generates a corresponding AppWrapper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and configure our cluster object (and appwrapper)\n",
    "cluster = Cluster(ClusterConfiguration(\n",
    "    name='raytest',\n",
    "    namespace='default',\n",
    "    min_worker=1,\n",
    "    max_worker=1,\n",
    "    min_cpus=2,\n",
    "    max_cpus=2,\n",
    "    min_memory=8,\n",
    "    max_memory=8,\n",
    "    image=\"quay.io/project-codeflare/ray:2.5.0-py38-cu116\", #optional, default is ray 2.1.0\n",
    "    gpu=0,\n",
    "    instascale=False,\n",
    "    local_interactive=False #set to True if using cluster interactively from local machine, requires auth\n",
    "))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to bring our cluster up, so we call the `up()` function below to submit our cluster AppWrapper yaml onto the MCAD queue, and begin the process of obtaining our resource cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring up the cluster\n",
    "cluster.up()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to check on the status of our resource cluster, and wait until it is finally ready for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.wait_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.status()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly verify that the specs of the cluster are as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.details()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our cluster, there are two ways we can use it:\n",
    " - Submit direct batch jobs via TorchX\n",
    " - Develop interactively with Ray remote function execution\n",
    "\n",
    " Let's start with the direct, fire-and-forget job submission approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobdef = DDPJobDefinition(\n",
    "    name=\"mnisttest\",\n",
    "    script=\"mnist.py\",\n",
    "    scheduler_args={\"requirements\": \"requirements.txt\"}\n",
    ")\n",
    "job = jobdef.submit(cluster)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at the status of our submitted job, as well as the logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.logs()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also go to the dashboard of our Ray cluster to see additional details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.cluster_dashboard_uri()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second approach is interacting with your Ray cluster cell-by-cell, by having local functions executed on your remote resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "# reset the ray context in case there's already one. \n",
    "ray.shutdown()\n",
    "\n",
    "# establish connection to ray cluster\n",
    "\n",
    "#install additionall libraries that will be required for model training\n",
    "runtime_env = {\"pip\": [\"transformers\", \"datasets\", \"evaluate\", \"pyarrow<7.0.0\", \"accelerate\"]}\n",
    "ray.init(address=cluster.cluster_uri(), runtime_env=runtime_env)\n",
    "\n",
    "print(\"Ray cluster is up and running: \", ray.is_initialized())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are connected to the Ray cluster, we can submit functions to be executed remotely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_fn():\n",
    "    from datasets import load_dataset, load_metric\n",
    "    import numpy as np\n",
    "\n",
    "    import transformers\n",
    "    from transformers import AutoTokenizer, TrainingArguments\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    \n",
    "    import ray\n",
    "    from ray.train.huggingface import HuggingFaceTrainer\n",
    "    from ray.air.config import ScalingConfig\n",
    "\n",
    "    # Dataset retrieval/preprocessing\n",
    "\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    #using a fraction of dataset but you can run with the full dataset\n",
    "    small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(100))\n",
    "    small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(100))\n",
    "\n",
    "    print(f\"len of train {small_train_dataset} and test {small_eval_dataset}\")\n",
    "\n",
    "    ray_train_ds = ray.data.from_huggingface(small_train_dataset)\n",
    "    ray_evaluation_ds = ray.data.from_huggingface(small_eval_dataset)\n",
    "\n",
    "\n",
    "    # Assessment metrics\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        metric = load_metric(\"accuracy\")\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Training\n",
    "\n",
    "    def trainer_init_per_worker(train_dataset, eval_dataset, **config):\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "        training_args = TrainingArguments(\"/tmp/hf_imdb/test\", eval_steps=1, disable_tqdm=True, \n",
    "                                          num_train_epochs=1, skip_memory_metrics=True,\n",
    "                                          learning_rate=2e-5,\n",
    "                                          per_device_train_batch_size=4,\n",
    "                                          per_device_eval_batch_size=4,                                \n",
    "                                          weight_decay=0.01,)\n",
    "        return transformers.Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "    scaling_config = ScalingConfig(num_workers=2, use_gpu=False) #num workers is the number of gpus\n",
    "\n",
    "    # we are using the ray native HuggingFaceTrainer, but you can swap out to use non ray Huggingface Trainer. Both have the same method signature. \n",
    "    # the ray native HFTrainer has built in support for scaling to multiple GPUs\n",
    "    trainer = HuggingFaceTrainer(\n",
    "        trainer_init_per_worker=trainer_init_per_worker,\n",
    "        scaling_config=scaling_config,\n",
    "        datasets={\"train\": ray_train_ds, \"evaluation\": ray_evaluation_ds},\n",
    "    )\n",
    "    result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the above cell as a remote ray function\n",
    "ray.get(train_fn.remote())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To disconnect at any time, simply call the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once complete, we can bring our Ray cluster down and clean up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.down()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
